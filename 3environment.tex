\chapter{Environment}
\label{chapter:environment}

Here we describe the environment we work in. We describe the cluster used for testing, the software deployed and the dataset of our experiments.

\section{Triton}

For this final project we have been using Triton for testing solutions as well as doing performance evaluation tests. Triton is a high performance cluster owned by Aalto University School of Science and used by its members \cite(wiki de triton).

As of July 2013, Triton has 238 compute nodes divided in 5 big groups:

\begin{enumerate}
\item Frontend node: HP SL390s G7 with 48GB of memory. The login node through which rest of the cluster is accessible to users.
\item 109 compute nodes HP ProLiant BL465c G6, each equipped with 2x Six-Core AMD Opteron 2435 2.6GHz processors. 80 compute nodes cn[01-80] have 32GB (cn[65-67] used for NFS servers needs), 32 others have 64GB cn[81-112], 4xDDR Infiniband port and local SATA drive with diskspace available ~215GB.
\item 118 compute nodes cn[113-224], tb[003-008] are HP SL390s G7, each equiped with 2x Intel Xeon X5650 2.67GHz (Westmere six-core each). Every SL390s G7 node has 48 GB of DDR3-1066 memory, 4xQDR Infiniband port and about 830 GB of local diskspace (2 mirrored drives). 16 nodes have by two additional SATA drives.
\item 8 compute nodes gpu[001-008] are HP SL390s G7 for gpu computing.
\item 2 fat nodes HP DL580 G7 4U, 4x Xeon, 6x SATA drives, 1TB of DDR3-1066 memory each and 4xQDR Infiniband port.
\end{enumerate}

Triton has two internal networks: Infiniband for MPI and Lustre filesystem and Gigabit Ethernet for everything else.

About storage, all nodes are connected to DDN SFA10k storage system: large disk arrays with the Lustre filesystem on top of it.

Triton runs Scientific Linux 6 and SLURM as a scheduler and batch system.

\bigskip
While the number of nodes used for each experiment in this project has been changing according to the needs, the used nodes have always been Intel Xeon and the Gigabit Ethernet internal network has been our choice as networking. 

\begin{table}[htbp]
\caption{}
\begin{tabular}{|l|l|}
\hline
Processor &  2x Intel Xeon X5650 2.67GHz \\ \hline
RAM  & 48 GB of DDR3-1066 memory \\ \hline
Storage  & About 830 GB of local diskspace (software RAID 0) \\ \hline
Chassis/Mobo  & HP SL390s G7 \\ \hline
\end{tabular}
\label{}
\end{table}


\section{Cloudera's Distribution Including Apache Hadoop - CDH}
For the purpose of this project, we have used Cloudera \cite{Cloudera} open source solution CDH.
As Cloudera states in their website \cite{ClouderaCDH}, Cloudera's Distribution Including Apache Hadoop (CDH) is one of the most used and tested distribution of Apache Hadoop. CDH is open source and is backed by Cloudera's organization. CDH contains the core elements of Hadoop, all of them tested and integrated.
\par
In 26 February 2013, coinciding with the start of this project, Cloudera released CDH 4.1.3, which included HBase 0.92.1, the last stable version of HBase with a bunch of fixes, Hadoop 2.0 along with lot of fixes and Apache MapReduce version one (MRv1). This is the software used although now you can find newer versions in Cloudera website.

\section{MySQL}

MySQL \cite{MySQL} has been chosen as the open source RDBMS to test against HBase. The version used for the experiments has been MySQL Community Server 5.6.12, which is the last stable release offered by MySQL.

\subsection{Yahoo! Cloud Serving Benchmark - YCSB}

YCSB\cite{cooper2010benchmarking} is a standard open-source benchmark tool developed by Yahoo! Labs, its aim is to provide a general framework for evaluating the performance of distributed key/value and cloud storage systems, such as HBase, Cassandra, and PNUTS. YCSB allows one to define different workload scenarios by mixing reads, writes, updates and table scans, and then measures the performance of the system on a particular workload.
\par
\fixme{Screenshot from 2013-08-15 14:34:45.png }
\par
YCSB lastest version \cite{YCSB} used: ycsb 0.1.4.

\section{Hannibal}
\fixme{should I talk about Hannibal tool for monitoring..as well as DStats for plotting?} 





\subsection{HBase schema design}

Here we show the Hbase table schema design used to store the data into our HBase cluster. Reader can see how the previous XML video data would look stored into the table: Our row key is the \textit{uid} of the video. There are 4 column families: \textit{main}, \textit{genre}, \textit{tag} and \textit{resource}. Each column family contains many column qualifiers.
\par
By convention, a column name is made of its column family prefix and a qualifier (Ex. main:description). The colon character delimits both columns.


\begin{table}[htbp]

\begin{center}
\begin{sideways}
\scalebox{0.30}[1]{
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
Row Key & Time Stamp & main:author & main:albumArt & main:description & main:mediaDate & main:serviceName & main:externalUrl & main:videoName & main:sourceRating & Tag:n & genre:n & resource:streamUrl:n & resource:duration:n & resource:itemTypeId:n & resource:mimeType:n & resource:resourceType:n & resource:uid:n & resource:width:n & resource:height:n \\ \hline
f5e477ba4d & t5 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & \multicolumn{1}{r|}{450} \\ \hline
f5e477ba4d & t3 & karhu &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \hline
f5e477ba4d & t2 &  & http://assets0.ordienetworks.com/tmbs/f5e477ba4d/fullsize\_9.jpg.. & painful &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \hline
f5e477ba4d & t1 & paropetje & http://... & feel the adrenaline & 2012-05-12 21:02:49.0 & Funnyordie & http://... & toys for boys &  & toys & Real Life & http://... & 0:01:42 & \multicolumn{1}{r|}{1} & application/x-shockwave-flash & stream & f5e477ba4d & \multicolumn{1}{r|}{640} & \multicolumn{1}{r|}{400} \\ \hline
\end{tabular}}
\end{sideways}
\end{center}
\caption{Our HBase table schema with a sample stored. Different timestamps due to updates.}
\label{HTable}
\end{table}


\subsection{Our dataset}

Our dataset is composed of thousands of XML files whose sizes vary from just a few KBs to dozens of MBs. Each of them is composed in turn of a list of elements wrapped between <element></element> tags. Every element is constituted of at least 15 sub-elements, which can be any kind of raw data (strings, integers, etc) or the start point of another sub-list. While the total size of the XML is known, the amount of elements within each document is unknown due to the unfixed size of the XML files. The same situation happens with the size of each element, the number of sub-elements is unfixed as well as the size of each one.
\par
A conceptual example of one of our XML files is shown below \footnote{For the sake of simplicity, only a basic form of a real XML is depicted.}.

\lstset{language=XML, basicstyle=\footnotesize, numbers=left, breaklines=true}
\begin{lstlisting}
<element 1>
...
</element 1>
<element 2>
	<sub-element 1>
		"Hi, I am a looooooong string"
	</sub-element 1>
	...
	<sub-element n>
		<sublist1>
			...
		</sublist1>
	</sub-element n>
</element 2>
...
<element 3>
...
</element 3>
\end{lstlisting}

\bigskip

A real example can be found in the appendix 1. It will be useful in order to understand the HBase table design described 

\subsection{HBase schema design}

In the following chapters we will work with HBase as our datastore. For which reason, we have designed an HBase schema that is able to hold our XML data previously depicted. It is worth to state that this schema is focused on obtaining a high performance when updating and retrieving data from it; our row key is the element \textit{uid} of an element since most of the request we will have to cope with are requests of some \cite{uids}. Nonetheless, there could be requests of some other nature, but they will only represent a low percent of the total requests.

 and that is why it has that structure and no other.
\par
In the appendix 2, we show the followed Hbase table schema design. The reader can see how the previous XML video data would look stored into the table: Our row key is the \textit{uid} of the video. There are 4 column families: \textit{main}, \textit{genre}, \textit{tag} and \textit{resource}. Each column family contains many column qualifiers.
\par
By convention, a column name is made of its column family prefix and a qualifier (Ex. main:description). The colon character delimits both columns.



