\chapter{Environment}
\label{chapter:environment}

Here we describe the environment we work in. We describe the cluster used for testing, the software deployed and the dataset of our experiments.

\section{Triton}

For this final project we have been using Triton for testing solutions as well as doing performance evaluation tests. Triton is a high performance cluster owned by Aalto University School of Science and used by its members \cite(wiki de triton).

As of July 2013, Triton has 238 compute nodes divided in 5 big groups:

\begin{enumerate}
\item Frontend node: HP SL390s G7 with 48GB of memory. The login node through which rest of the cluster is accessible to users.
\item 109 compute nodes HP ProLiant BL465c G6, each equipped with 2x Six-Core AMD Opteron 2435 2.6GHz processors. 80 compute nodes cn[01-80] have 32GB (cn[65-67] used for NFS servers needs), 32 others have 64GB cn[81-112], 4xDDR Infiniband port and local SATA drive with diskspace available ~215GB.
\item 118 compute nodes cn[113-224], tb[003-008] are HP SL390s G7, each equiped with 2x Intel Xeon X5650 2.67GHz (Westmere six-core each). Every SL390s G7 node has 48 GB of DDR3-1066 memory, 4xQDR Infiniband port and about 830 GB of local diskspace (2 mirrored drives). 16 nodes have by two additional SATA drives.
\item 8 compute nodes gpu[001-008] are HP SL390s G7 for gpu computing.
\item 2 fat nodes HP DL580 G7 4U, 4x Xeon, 6x SATA drives, 1TB of DDR3-1066 memory each and 4xQDR Infiniband port.
\end{enumerate}

Triton has two internal networks: Infiniband for MPI and Lustre filesystem and Gigabit Ethernet for everything else.

About storage, all nodes are connected to DDN SFA10k storage system: large disk arrays with the Lustre filesystem on top of it.

Triton runs Scientific Linux 6 and SLURM as a scheduler and batch system.

\bigskip
While the number of nodes used for each experiment in this project has been changing according to the needs, the used nodes have always been Intel Xeon and the Gigabit Ethernet internal network has been our choice as networking. 

\begin{table}[htbp]
\caption{}
\begin{tabular}{|l|l|}
\hline
Processor &  2x Intel Xeon X5650 2.67GHz \\ \hline
RAM  & 48 GB of DDR3-1066 memory \\ \hline
Storage  & About 830 GB of local diskspace (software RAID 0) \\ \hline
Chassis/Mobo  & HP SL390s G7 \\ \hline
\end{tabular}
\label{}
\end{table}


\section{Cloudera's Distribution Including Apache Hadoop - CDH}
For the purpose of this project, we have used Cloudera \cite{Cloudera} open source solution CDH.
As Cloudera states in their website \cite{ClouderaCDH}, Cloudera's Distribution Including Apache Hadoop (CDH) is one of the most used and tested distribution of Apache Hadoop. CDH is open source and is backed by Cloudera's organization. CDH contains the core elements of Hadoop, all of them tested and integrated.
\par
In 26 February 2013, coinciding with the start of this project, Cloudera released CDH 4.1.3, which included HBase 0.92.1, the last stable version of HBase with a bunch of fixes, Hadoop 2.0 along with lot of fixes and Apache MapReduce version one (MRv1). This is the software used although now you can find newer versions in Cloudera website.

\section{MySQL}

MySQL \cite{MySQL} has been chosen as the open source RDBMS to test against HBase. The version used for the experiments has been MySQL Community Server 5.6.12, which is the last stable release offered by MySQL.

\subsection{Yahoo! Cloud Serving Benchmark - YCSB}

YCSB\cite{cooper2010benchmarking} is a standard open-source benchmark tool developed by Yahoo! Labs, its aim is to provide a general framework for evaluating the performance of distributed key/value and cloud storage systems, such as HBase, Cassandra, and PNUTS. YCSB allows one to define different workload scenarios by mixing reads, writes, updates and table scans, and then measures the performance of the system on a particular workload.
\par
\fixme{Screenshot from 2013-08-15 14:34:45.png }
\par
YCSB lastest version \cite{YCSB} used: ycsb 0.1.4.

\section{Hannibal}
\fixme{should I talk about Hannibal tool for monitoring..as well as DStats for plotting?} 





\section{Dataset PV.com}

Here we show an example of an XML video file:
Basically, it includes a list of videos, each one represented between tags <video>...</video>. Our dataset consists of thousand of these XML files.

\lstset{language=XML, basicstyle=\footnotesize, numbers=left, breaklines=true}
\begin{lstlisting}
<index><videoList>
<video>
	<albumArt>http://assets0.ordienetworks.com/tmbs/f5e477ba4d/fullsize_8.jpg</albumArt>
	<author>
		<userName>paropetje</userName>
	</author>
	<description>feel the adrenaline</description>
	<mediaDate>2012-05-12 21:02:49.0</mediaDate>
	<uid>f5e477ba4d</uid>
	<explicit>false</explicit>
	<serviceLabel>FunnyOrDie</serviceLabel>
	<externalUrl>http://www.funnyordie.com/videos/f5e477ba4d</externalUrl>
	<videoName>toys for boys</videoName>
	<genreList>
		<genre>Real Life</genre>
	</genreList>
	<isResourceList>true</isResourceList>
	<resourceList>
		<resource>
			<streamUrl>http://videos0.ordienetworks.com/videos/f5e477ba4d/sd.flv</streamUrl>
			<duration>0:01:42</duration>
			<itemTypeId>1</itemTypeId>
			<mimeType>application/x-shockwave-flash</mimeType>
			<resourceType>stream</resourceType>
			<uid>http://videos0.ordienetworks.com/videos/f5e477ba4d/sd.flv</uid>
			<height>400</height>
			<width>640</width>
			<formatList>
				<format>
					<height>400</height>
					<width>640</width>
					<mimeType>application/x-shockwave-flash</mimeType>
				</format>
			</formatList>
		</resource>
		<resource>
			<streamUrl>http://videos0.ordienetworks.com/videos/f5e477ba4d/iphone_wifi.mp4</streamUrl>
			<duration>0:01:42</duration>
			<itemTypeId>1</itemTypeId>
			<mimeType>video/mp4</mimeType>
			<resourceType>stream</resourceType>
			<uid>http://videos0.ordienetworks.com/videos/f5e477ba4d/iphone_wifi.mp4</uid>
			<height>400</height>
			<width>640</width>
			<formatList>
				<format>
					<height>400</height>
					<width>640</width>
					<mimeType>video/mp4</mimeType>
				</format>
			</formatList>
		</resource>
	</resourceList>
	<tagList>
		<tag>toys</tag>
		<tag>boys</tag>
		<tag>motor</tag>
		<tag>moter</tag>
		<tag>bike</tag>
		<tag>adrenaline</tag>
		<tag>kick</tag>
	</tagList>
	<serviceName>Funnyordie</serviceName>
	<sourceViewCount>1</sourceViewCount>
</video>
<video>
...
</video>
</videoList></index>
\end{lstlisting}


\subsection{HBase schema design}

Here we show the Hbase table schema design used to store the data into our HBase cluster. Reader can see how the previous XML video data would look stored into the table: Our row key is the \textit{uid} of the video. There are 4 column families: \textit{main}, \textit{genre}, \textit{tag} and \textit{resource}. Each column family contains many column qualifiers.
\par
By convention, a column name is made of its column family prefix and a qualifier (Ex. main:description). The colon character delimits both columns.


\begin{table}[htbp]

\begin{center}
\begin{sideways}
\scalebox{0.30}[1]{
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
Row Key & Time Stamp & main:author & main:albumArt & main:description & main:mediaDate & main:serviceName & main:externalUrl & main:videoName & main:sourceRating & Tag:n & genre:n & resource:streamUrl:n & resource:duration:n & resource:itemTypeId:n & resource:mimeType:n & resource:resourceType:n & resource:uid:n & resource:width:n & resource:height:n \\ \hline
f5e477ba4d & t5 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & \multicolumn{1}{r|}{450} \\ \hline
f5e477ba4d & t3 & karhu &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \hline
f5e477ba4d & t2 &  & http://assets0.ordienetworks.com/tmbs/f5e477ba4d/fullsize\_9.jpg.. & painful &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \hline
f5e477ba4d & t1 & paropetje & http://... & feel the adrenaline & 2012-05-12 21:02:49.0 & Funnyordie & http://... & toys for boys &  & toys & Real Life & http://... & 0:01:42 & \multicolumn{1}{r|}{1} & application/x-shockwave-flash & stream & f5e477ba4d & \multicolumn{1}{r|}{640} & \multicolumn{1}{r|}{400} \\ \hline
\end{tabular}}
\end{sideways}
\end{center}
\caption{Our HBase table schema with a sample stored. Different timestamps due to updates.}
\label{HTable}
\end{table}


\subsectiobn{Our dataset}

