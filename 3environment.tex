\chapter{Environment}
\label{chapter:environment}

Here we describe the environment we has worked in. We describe the cluster used for testing, the software deployed and the dataset of our experiments.

\section{Triton}

For this final project we have been using Triton for testing solutions as well as doing performance evaluation tests. Triton is a high performance cluster owned by Aalto University School of Science and used by its members \cite(wiki de triton).

As of July 2013, Triton has 238 compute nodes divided in 5 big groups:

\begin{enumerate}
\item Frontend node: HP SL390s G7 with 48GB of memory. The login node through which rest of the cluster is accessible to users.
\item 109 compute nodes HP ProLiant BL465c G6, each equipped with 2x Six-Core AMD Opteron 2435 2.6GHz processors. 80 compute nodes cn[01-80] have 32GB (cn[65-67] used for NFS servers needs), 32 others have 64GB cn[81-112], 4xDDR Infiniband port and local SATA drive with diskspace available ~215GB.
\item 118 compute nodes cn[113-224], tb[003-008] are HP SL390s G7, each equiped with 2x Intel Xeon X5650 2.67GHz (Westmere six-core each). Every SL390s G7 node has 48 GB of DDR3-1066 memory, 4xQDR Infiniband port and about 830 GB of local diskspace (2 mirrored drives). 16 nodes have by two additional SATA drives.
\item 8 compute nodes gpu[001-008] are HP SL390s G7 for gpu computing.
\item 2 fat nodes HP DL580 G7 4U, 4x Xeon, 6x SATA drives, 1TB of DDR3-1066 memory each and 4xQDR Infiniband port.
\end{enumerate}

Triton has two internal networks: Infiniband for MPI and Lustre filesystem and Gigabit Ethernet for everything else.

About storage, all nodes are connected to DDN SFA10k storage system: large disk arrays with the Lustre filesystem on top of it.

Triton runs Scientific Linux 6 and SLURM as a scheduler and batch system.

\bigskip
While the number of nodes used for each experiment in this project has been changing according to the needs, the used nodes have always been Intel Xeon while the Gigabit Ethernet internal network has been our choice as networking.

\begin{table}[htbp]
\caption{}
\begin{tabular}{|l|l|}
\hline
Processor &  2x Intel Xeon X5650 2.67GHz \\ \hline
RAM  & 48 GB of DDR3-1066 memory \\ \hline
Storage  & About 830 GB of local diskspace (software RAID 0) \\ \hline
Chassis/Mobo  & HP SL390s G7 \\ \hline
\end{tabular}
\label{}
\end{table}


\section{Cloudera's Distribution Including Apache Hadoop - CDH}
For the purpose of this project, we have used Cloudera \cite{Cloudera} open source solution CDH.
As Cloudera states in their website \cite{ClouderaCDH}, Cloudera's Distribution Including Apache Hadoop (CDH) is one of the most used and tested distribution of Apache Hadoop. CDH is open source and is backed by Cloudera's organization. CDH contains the core elements of Hadoop, all of them tested and integrated.
\par
In 26 February 2013, coinciding with the start of this project, Cloudera released CDH 4.1.3, which included HBase 0.92.1, the last stable version of HBase with a bunch of fixes, Hadoop 2.0 along with lot of fixes and Apache MapReduce version one (MRv1). This is the software used although now you can find newer versions in Cloudera website.

\section{MySQL}

MySQL \cite{MySQL} has been chosen as the open source RDBMS to test against HBase. The version used for the experiments has been MySQL Community Server 5.6.12, which is the last stable release offered by MySQL.

\section{Yahoo! Cloud Serving Benchmark - YCSB}

YCSB\cite{cooper2010benchmarking} is a standard open-source benchmark tool developed by Yahoo! Labs, its aim is to provide a general framework for evaluating the performance of distributed key/value and cloud storage systems, such as HBase, Cassandra, and PNUTS. YCSB allows one to define different workload scenarios by mixing reads, writes, updates and table scans, and then measures the performance of the system on a particular workload.
\par
\fixme{Screenshot from 2013-08-15 14:34:45.png }
\par
YCSB lastest version \cite{YCSB} used: ycsb 0.1.4.

\section{Hannibal}

\fixme{should I talk about Hannibal tool for monitoring..as well as DStats for plotting?} 



\section{Introducing our Dataset}

Our dataset is composed of thousands of XML files (more than 106GB of data) whose sizes vary from just a few KBs to dozens of MBs. Each of them is composed in turn of a list of elements wrapped between <element></element> tags. Every element is constituted of at least 15 sub-elements which are either integers or strings, or the start point to another sub-list of N sub-elements. While the total size of the XML is known, the amount of elements within each document is unknown due to the unfixed size of the XML files. The same situation happens with the size of each element where its number of sub-elements is unfixed as well as the length of each one (one can be a string sentence really long while other an integer). 
\par
A conceptual example of one of our XML files is shown below \footnote{For the sake of simplicity, only a basic form of a real XML is depicted.}.

\lstset{language=XML, basicstyle=\footnotesize, numbers=left, breaklines=true}
\begin{lstlisting}
<element 1>
...
</element 1>
<element 2>
	<sub-element 1>
		"Hi, I am a looooooong string"
	</sub-element 1>
	...
	<sub-element n>
		<sublist1>
			...
		</sublist1>
	</sub-element n>
</element 2>
...
<element N>
...
</element N>
\end{lstlisting}

\bigskip

A real XML sample can be found in the Appendix 1. It is useful in order to understand the HBase storage schema proposed and described in the next subsection.

\subsection{HBase storage schema design}

In the following chapters we will work with HBase as our datastore. For which reason, we have designed an HBase storage schema that is able to map our XML data previously depicted to our HBase database. 

\par
The sparse nature of HBase tables (not all columns populated in a row) makes them an interesting storage substitute for our XML dataset, in which elements can have different number of items, or even different items.

\par
It is worth to state that this schema is based on our main data access pattern in order to support efficient performance when updating and retrieving data from it; our row key is the \textit{Uid} tag \footnote{Uid and other real name's elements included in the Appendix 1 -  XML sample instance} since most of the request we will have to cope with will be row key-based requests. Nonetheless, there could be retrievals of some other nature, but they will only represent a low percent of the total requests.
\bigskip

In the Appendix 2, we show the followed Hbase storage schema design. The reader will be able to see how the previous XML video data maps into our Hbase database: Our row key is the \textit{uid} of the video. There are 4 column families: \textit{main}, \textit{genre}, \textit{tag} and \textit{resource}. Each column family contains many column qualifiers.
\par
By convention, a column name is made of its column family prefix and a qualifier (Ex. main:description). The colon character delimits both columns.



